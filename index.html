<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Prediction assignment : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Prediction assignment</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/abagherp/Prediction_Assignment">View on GitHub</a>

          <h1 id="project_title">Prediction assignment</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/abagherp/Prediction_Assignment/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/abagherp/Prediction_Assignment/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p></p>

<p></p>

<p></p>Prediction Assignment



<p>
</p>







code{white-space: pre;}

<p></p>




  pre:not([class]) {
    background-color: white;
  }




<p></p>

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}


<div>


<div id="header">
<h1>
<a id="prediction-assignment" class="anchor" href="#prediction-assignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prediction Assignment</h1>
<h4>
<a id="ali-bagherpour" class="anchor" href="#ali-bagherpour" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>Ali Bagherpour</em>
</h4>
<h4>
<a id="august-22-2015" class="anchor" href="#august-22-2015" aria-hidden="true"><span class="octicon octicon-link"></span></a><em>August 22, 2015</em>
</h4>
</div>

<div id="executive-summary">
<h1>
<a id="executive-summary" class="anchor" href="#executive-summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Executive summary</h1>
<p>Nowadays, wearable devices are getting better and better in providing useful information about many aspects of our life. Among these, smart watches and smart wristbands are getting more and more popular between customers because these devices can monitor, detect and report the activities that their owner is doing while wearing these devices.</p>
<p>An interesting feature for owners of these devices is to get feedback about how well they performed their activity. In this project, we investigate the possibility of detecting <em>correct</em> and <em>incorrect</em> barbell lifts preformed by participant that wear accelerometers on the belt, forearm, arm, and dumbell. The 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information on data is available from <a href="http://groupware.les.inf.puc-rio.br/har">here</a>.</p>
</div>

<div id="expolratory-analysis">
<h1>
<a id="expolratory-analysis" class="anchor" href="#expolratory-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Expolratory analysis</h1>
<pre><code>library(caret)
library(randomForest)</code></pre>
<p>First we load caret library to be able to perform machine learning algorithms. Next we need to load data into R. The train and test data were provided in two csv files. They are imported as data frames.</p>
<pre><code>## Loading data into R
train_data = read.csv("pml-training.csv")
test_data = read.csv("pml-testing.csv")</code></pre>
<p>The train data has 160 columns and 19622 rows. The columns are the variables that are monitored during each test. Each row represents one experiment. The last column o train_data, “classe”, is the response that we want to predict.</p>
<p>By looking at test data we can see many columns have “NA” data and can’t be used for prediction of the response. We remove these columns from train_data and test_data since they won’t be useful in the model.</p>
<pre><code>## Remove columns with NA data in test_data 
train_data = train_data[, colSums(is.na(test_data)) != nrow(test_data)]
test_data = test_data[, colSums(is.na(test_data)) != nrow(test_data)]</code></pre>
<p>The first 7 columns of the dataset are the identifiers of the experiments and participants and will be removed from the data.</p>
<pre><code>train_data = train_data[-c(1:7)]
test_data = test_data[-c(1:7)]</code></pre>
<p>To implement and test the model we split data into train (70%) and test (30%).</p>
<pre><code>inTrain = createDataPartition(y=train_data$classe,p=0.7)[[1]]
training = train_data[inTrain,]
testing = train_data[-inTrain,]</code></pre>
</div>

<div id="variable-selection">
<h1>
<a id="variable-selection" class="anchor" href="#variable-selection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variable Selection</h1>
<p>The training data set now has 53 columns and 13737 rows. An important part of any machine learning algorithm is to find important variables that help predict the response. There are many ways to achieve this. The method here used is to identify variables that are highly corrolated and remove one of them. <em>findCorrelation</em> function looks at the correlation matrix and if two variables have a correlation higher than the cutoff value (here 0.7 was chosen) it removes the variable with the largest mean absolute correlation.</p>
<pre><code>colClass = sapply(testing[,1:ncol(testing)-1],class)
testing[colClass=="factor"] = sapply(testing[colClass=="factor"],as.numeric)
training[colClass=="factor"] = sapply(training[colClass=="factor"],as.numeric)
training.scale&lt;- scale(training[,!names(training) %in% "classe"],center=TRUE,scale=TRUE);
corMatMy &lt;- cor(training.scale)
highlyCor &lt;- findCorrelation(corMatMy, 0.70)
#Apply correlation filter at 0.70,
#then we remove all the variable correlated with more 0.7.
training= training[,-highlyCor]
testing = testing[,-highlyCor]</code></pre>
<p>By performing this filtering algorithm the training data set now has 31 columns and 13737 rows.</p>
<p>Now to find a model for the response data we use a random forest method. In preliminary trys of fitting these model it was found that default values of the random forest create a very time consuming procedure. The important feature was found to be the number of trees to grow, <em>ntree</em>. By try and error ntree=80 was chosen. The dafault value was ntree=500.</p>
<pre><code># train_control &lt;- trainControl(method="cv", number=10)
modfit = randomForest(classe ~ ., data=training ,ntree=50,proximity = TRUE)
print(modfit)</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = classe ~ ., data = training, ntree = 50,      proximity = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 50
## No. of variables tried at each split: 5
## 
##         OOB estimate of  error rate: 1.66%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3896    7    1    1    1 0.002560164
## B   39 2598   15    3    3 0.022573363
## C    5   36 2336   18    1 0.025041736
## D    2    2   64 2171   13 0.035968028
## E    2    3    4    8 2508 0.006732673</code></pre>
<p>The important variables to predit the response class can be checked</p>
<pre><code>varImpPlot(modfit)</code></pre>
<p><img title alt width="672"></p>
<p>Now we can use this model to predict the classe variable in the testing dataset.</p>
<pre><code>pred &lt;- predict(modfit , testing[,1:ncol(testing)-1])
confusionMatrix(testing$classe , pred)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1667    7    0    0    0
##          B   16 1113   10    0    0
##          C    0   10 1003   12    1
##          D    0    1   21  939    3
##          E    0    0    1    2 1079
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9857          
##                  95% CI : (0.9824, 0.9886)
##     No Information Rate : 0.286           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9819          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9905   0.9841   0.9691   0.9853   0.9963
## Specificity            0.9983   0.9945   0.9953   0.9949   0.9994
## Pos Pred Value         0.9958   0.9772   0.9776   0.9741   0.9972
## Neg Pred Value         0.9962   0.9962   0.9934   0.9972   0.9992
## Prevalence             0.2860   0.1922   0.1759   0.1619   0.1840
## Detection Rate         0.2833   0.1891   0.1704   0.1596   0.1833
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9944   0.9893   0.9822   0.9901   0.9978</code></pre>
<p>As it can be seen from the table above, out of bag sample is around 1.5%.</p>
</div>

<p></p>
</div>







<p>
</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Prediction assignment maintained by <a href="https://github.com/abagherp">abagherp</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
